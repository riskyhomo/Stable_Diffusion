{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RwH-Giu8rPM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import math\n",
        "def __init__(self, n_heads, d_embed, in_proj_bias=True, out_proj_bias=True):\n",
        "  super.__init__()\n",
        "  self.in_proj_bias= nn.Linear(d_embed, 3*d_embed, bias=in_proj_bias)\n",
        "  self.out_proj_bias =nn.Linear(d_embed, d_embed, bias=out_proj_bias)\n",
        "  self.n_heads=n_heads\n",
        "  self.d_heads=d_embed//n_heads\n",
        "def forward(self, x, casual_mask=True):\n",
        "  input_shape=x.shape\n",
        "  batch_size, sequence_length, d_embed = input_shape\n",
        "  interium_shape=(batch_size, sequence_length, self.d_embed, self.d_heads)\n",
        "  q, k ,v= self.in_proj_bias(x).chunk(3, dim=-1)\n",
        "  q=q.view(interium_shape).transpose(1,2)\n",
        "  k=k.view(interium_shape).transpose(1,2)\n",
        "  v=v.view(interium_shape).transpose(1,2)\n",
        "  weight q @k.transpose(-1,-2)\n",
        "  if casual_mask:\n",
        "    mask=torch.ones_like(weight, dtype=torch.bool).triu(1)\n",
        "    weight.masked_fill_(mask, -torch.inf)\n",
        "  weight /=math.sqrt(self.d_head)\n",
        "  weight=F.softmax(weight, dim=-1)\n",
        "  output = weight @ v\n",
        "  output=output.transpose(1,2)\n",
        "  output = output.reshape(input_shape)\n",
        "  output= self.out_proj_bias(output)\n",
        "  return output\n",
        "class CrossAttention(nn.Module):\n",
        "  def __init__(self,n_heads, d_embed, d_cross, in_proj_bias =True, out_proj_bias=True)\n",
        "  super.__init__()\n",
        "  self.q_proj =nn.Linear(d_embed, d_embed, bias=in_proj_bias)\n",
        "  self.k_proj = nn.Linear(d_cross, d_embed, bias=in_proj_bias)\n",
        "  self.v_proj =nn.Linear(d_cross, d_embed, bias= in_proj_bias)\n",
        "  self.out_proj = nn.Linear(d_cross, d_embed, bias=out_proj_bias)\n",
        "  self.n_heads=n_heads\n",
        "  self.d_head= d_embed // n_heads\n",
        "\n",
        "def forward(self, x, y):\n",
        "  input_shape= x.shape\n",
        "  batch_size, sequence_length, d_embed = input_shape\n",
        "  interium_shape= (batch_size, -1, self.n_heads, self.d_head)\n",
        "  q= self.q_proj(x)\n",
        "  k=self.k_proj(x)\n",
        "  v=self.v_proj(x)\n",
        "\n",
        "  q=q.view(interium_shape).transpose(1,2)\n",
        "  k=k.view(interium_shape).transpose(1,2)\n",
        "  v=k.view(interium_shape).transpose(1,2)\n",
        "\n",
        "  weight = q @ k.transpose(-1, -2)\n",
        "  weight /= math.sqrt(self.d_head)\n",
        "  output = weight @v\n",
        "  output =output.transpose(1,2).contiguous()\n",
        "  output =output.view(input_shape)\n",
        "  output =self.out_proj(output)\n",
        "  return output\n",
        "\n"
      ]
    }
  ]
}